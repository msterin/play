############################################
#            Data sources                  #
############################################
<source>
  @type tail
  tag docker.*
  path /var/lib/docker/containers/*/*-json.log
  pos_file /tmp/fluentd/docker.pos
  format json
  refresh_interval 10s
  time_key time
  time_format %FT%T
</source>

<source>
  @type tail
  tag messages
  path /hostfs/var/log/messages
  pos_file /tmp/fluentd/messages.pos
  format none
  refresh_interval 10s
</source>

<source>
  @type tail
  tag secure
  path /hostfs/var/log/secure
  pos_file /tmp/fluentd/secure.pos
  format none
  refresh_interval 10s
</source>

<source>
  @type tail
  tag mesos
  path /hostfs/var/log/mesos/*
  pos_file /tmp/fluentd/mesos.pos
  format none
  refresh_interval 10s
</source>

<source>
  @type tail
  tag audit
  path /hostfs/var/log/audit/*
  pos_file /tmp/fluentd/audit.pos
  format none
  refresh_interval 10s
</source>

# MariaDB slow query log.

# Example slow log message:
#
# # Time: 171025 21:23:57
# # User@Host: root[root] @ localhost []
# # Thread_id: 42  Schema:   QC_hit: No
# # Query_time: 12.788064  Lock_time: 0.000000  Rows_sent: 1  Rows_examined: 0
# # Rows_affected: 0
# SET timestamp=1508966637;
# select sleep(12);
<source>
  @type tail
  # Parses each slow query as a multi-line message.
  # Time, user, thread ID, query time, and rows affected are parsed as separate fields.
  # The slow query SQL is parsed as the message field.
  format multiline
  format_firstline /^# Time: /
  format1 /(?<mariadb_slow_time># Time:.*\n)/
  format2 /(?<mariadb_slow_user># User@Host:.*\n)/
  format3 /(?<mariadb_slow_thread_id># Thread_id:.*\n)/
  format4 /(?<mariadb_slow_query_time># Query_time:.*\n)/
  format5 /(?<mariadb_slow_rows_affected># Rows_affected: \d+\n)/
  format6 /(?<message>.*)/
  # Flushes tail plugin for multiline format, so we don't need to wait for the next message.
  multiline_flush_interval 5s
  # This assumes that the slow query log in MariaDB is enabled and has been configured to be
  # written to /var/lib/mysql/slow_query.log
  path /hostfs/data/volumes/mariadb/slow_query.log
  pos_file /tmp/fluentd/mariadb-slow-query.pos
  refresh_interval 10s
  tag mariadb-slow-query
</source>

############################################
#            Data filters                  #
############################################

#--- based on Eli's examples

# This creates 2 fields for getting more context on Docker containers,
# cont_id and cont_meta_data -- which is simply a JSON load of the
# config.v2.json created by the JSON-logging driver
<filter docker.**>
  @type record_transformer
   enable_ruby
   <record>
    cont_id ${tag.split('.')[5]}
    # --This is for adding the complete set of container metadata available and
    # loads it into a field but does not JSON parse it--
    cont_meta_data ${id = tag.split('.')[5]; IO.read("/var/lib/docker/containers/#{id}/config.v2.json")}

  #    type "docker-logs"
  </record>
</filter>

# This section actually parses the cont_meta_data field as JSON
# and attaches it to the record with the prefix dockermetadata.
<filter docker.**>
  @type parser
  format json
  key_name cont_meta_data
  reserve_data true
  hash_value_field dockermetadata
</filter>

# Code that pulls out a particular key -- in this case
# Docker Image and makes it the type
<filter docker.**>
  @type record_transformer
  enable_ruby
  <record>
    type ${record['dockermetadata']['Config']['Image'].split(':')[0]}
  </record>
</filter>


# This code processes multi-line messages such as
# stack-traces and corresponds to the stacktrace.examples provided here
#<filter docker.**>
#  @type concat
#  key log
#  stream_identity_key cont_id
#  multiline_start_regexp /^\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}.*/
#  continuous_line_regexp /^[\t\s]+at.*/
#</filter>

# This copies the FluentD field log over to message for increased usability
# within the Kibana Discover page
<match docker.**>
  @type record_reformer
  renew_record false
  enable_ruby false

  tag droprecords
  <record>
    message ${log}
  </record>
</match>

# This drops unnecessary keys and unusued Docker metadata.
# It also changes the FluentD tag to the 'type' for routing to multiple sub-accounts
<match droprecords>
  @type record_reformer
  remove_keys dockermetadata,cont_meta_data,log
  renew_record false
  enable_ruby false
  tag ${type}
</match>
#========= TBD: replace tghe section below and test

<filter docker.var.lib.docker.containers.**>
  @type record_transformer
  enable_ruby
  <record>
    cont_id ${tag.split('.')[5]}
    type "docker-logs"
    hostname ${hostname}
    cont_meta_data ${id = tag.split('.')[5]; IO.read("/var/lib/docker/containers/#{id}/config.v2.json")}
  </record>
</filter>

# Handle multiline logs generically with start of the multiline entry identified by "YYYY-MM-DD hh:mm:ss"
# This should handle all of the java processes built with ss-base libraries
# Ref. https://github.com/fluent-plugins-nursery/fluent-plugin-concat#usage
<filter docker.**>
  @type concat
  timeout_label @LOGZIO_BUFFERED
  key log
  stream_identity_key cont_id
  multiline_start_regexp /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2}/
  flush_interval 10s
</filter>

#RabbitMQ multiline log processing
<filter docker.**>
  @type concat
  timeout_label @ERROR
  key log
  stream_identity_key cont_id
  multiline_start_regexp /=.* REPORT====/
  multiline_end_regexp /^\n$/
  flush_interval 10s
</filter>

<label @ERROR>
  <match docker.log>
    @type stdout
  </match>
</label>

<filter **>
  @type record_transformer
  <record>
    tenant "sks"
    role "#{ENV['MESOS_ROLE']}"
    region "#{ENV['AWS_REGION']}"
  </record>
</filter>

<filter messages>
  @type record_transformer
    <record>
      hostname ${hostname}
      type "logs"
    </record>
</filter>

<filter audit>
  @type record_transformer
    <record>
      hostname ${hostname}
      type "logs"
    </record>
</filter>

<filter mesos>
  @type record_transformer
    <record>
      hostname ${hostname}
      type "logs"
    </record>
</filter>

<filter secure>
  @type record_transformer
    <record>
      hostname ${hostname}
      type "logs"
    </record>
</filter>

<filter mariadb-slow-query>
  @type record_transformer
    <record>
      hostname ${hostname}
      type "logs"
    </record>
</filter>

############################################
#       Data  matches and rewrites         #
############################################
# rewrite log field to message
<match docker.**>
  @type record_reformer
  renew_record false
  enable_ruby false
  tag lccolumn
  <record>
    message ${log}
  </record>
</match>

# remove the old log field after copying to message
<match lccolumn>
  @type record_reformer
  remove_keys log
  renew_record false
  enable_ruby false
  tag docker-logs
</match>

# Label all logs to be shipped to logz.io
<match **>
    @type relabel
    @label @LOGZIO_BUFFERED
</match>

# Ship logs with label @LOGZIO_BUFFERED to Logz.io
<label @LOGZIO_BUFFERED>
  <match **>
    @type logzio_buffered
    endpoint_url "https://listener.logz.io:8071?token=#{ENV['LOGZIO_API_TOKEN']}&type=logs"
    output_include_time true
    output_include_tags true
    buffer_type    file
    buffer_path    /tmp/fluentd/logsz_buffer
    flush_interval 10s
    buffer_chunk_limit 1m
  </match>
</label>

